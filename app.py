import os
import traceback
import streamlit as st
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.prompts import PromptTemplate

# ุชุญููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ
load_dotenv()
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

DB_FAISS_PATH = os.path.join(BASE_DIR, "vectorstore", "db_faiss")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
HF_TOKEN = os.getenv("HF_TOKEN")
# ุชุนุฏูู RTL ููุบุฉ ุงูุนุฑุจูุฉ
st.markdown(
    """
    <style>
    html, body, .main {
        direction: rtl;
        text-align: right;
    }
    .st-chat-message > div {
        direction: rtl;
        text-align: right;
    }
    .stTextInput>div>input {
        direction: rtl;
        text-align: right;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ุชุญููู ูุงุนุฏุฉ FAISS
@st.cache_resource
def get_vectorstore():
    """ุชุญููู ูุงุนุฏุฉ FAISS ูู ุงููุณุงุฑ ุงููุญุฏุฏ ูุน ุงูุชุญูู ูู ูุฌูุฏ ุงููููุงุช."""

    index_path = os.path.join(DB_FAISS_PATH, "index.faiss")
    meta_path = os.path.join(DB_FAISS_PATH, "index.pkl")

    if not os.path.isdir(DB_FAISS_PATH) or not (
        os.path.isfile(index_path) and os.path.isfile(meta_path)
    ):
        raise FileNotFoundError(
            "ูู ูุชู ุงูุนุซูุฑ ุนูู ูุงุนุฏุฉ FAISS. "
            "ุดุบููู faiss_build.py ูุญูููุง ุซู ุงุฑูุนู ูุฌูุฏ vectorstore/db_faiss ุฅูู ุงููุณุชูุฏุน "
            "ุฃู ุดุบูู CI ูุจูุงุฆูุง ุชููุงุฆููุง."
        )

    embeddings = HuggingFaceEndpointEmbeddings(
        model="sentence-transformers/all-MiniLM-L6-v2",
        task="feature-extraction",
        huggingfacehub_api_token=HF_TOKEN,
    )
    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)
    return db

# ูุงุฆูุฉ ุงูุชุญูุงุช
GREETINGS = ["ูุฑุญุจุง", "ุฃููุงู", "ุฃููุง", "ูุงู", "ุงูุณูุงู ุนูููู"]

# ูุงุฆูุฉ ุงูุฃุณุฆูุฉ ุงูููุชุฑุญุฉ ุจูุงุกู ุนูู ุงูุจูุงูุงุช
SUGGESTED_QUESTIONS = [
    "ูุง ูู ุฎุฏูุงุช ุชุทููุฑ ุงูุชุทุจููุงุช ุงูุชู ููุฏููุง ุงูููุชุจุ",
    "ูู ููุฏู ุงูููุชุจ ุญููู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุงูู Chatbotsุ",
    "ูุง ูู ุฃูุธูุฉ ERP ู CRM ุงูุชู ูุทูุฑูุง ุงูููุชุจุ",
    "ููู ูููู ุงูููุชุจ ุจุชุทููุฑ ุงููุชุงุฌุฑ ุงูุฅููุชุฑูููุฉุ",
    "ูุง ูู ุฎุฏูุงุช ุงูุฃูู ุงูุณูุจุฑุงูู ุงููุชุงุญุฉุ",
    "ูู ูููุฑ ุงูููุชุจ ุฎุฏูุงุช ุงุณุชุถุงูุฉ ูุณูุฑูุฑุงุชุ",
    "ูุง ูู ุงููุดุงุฑูุน ุงูุณุงุจูุฉ ููููุชุจุ",
    "ููู ูุชู ุชุทููุฑ ูุธุงู ุญุฌุฒ ุงูููุงุนูุฏ ุงูุทุจูุ",
    "ูุง ูู ุชูููุงุช ุงูุชุนูู ุงูุขูู ุงูุชู ูุณุชุฎุฏููุง ุงูููุชุจุ",
    "ูู ููุฏู ุงูููุชุจ ุฎุฏูุงุช ุงูุชุณููู ุงูุฑูููุ",
    "ูุง ูู ุงูููู ุงูุฃุณุงุณูุฉ ููููุชุจุ",
    "ููู ูุชู ุจูุงุก ููุตุฉ ุชุนููู ุฅููุชุฑูููุฉุ",
]

# Prompt ููุจูุช ุงูุฐูู + ุณุคุงู ูุชุงุจุนุฉ
retrieval_prompt = PromptTemplate(
    input_variables=["context", "input"],
    template="""
ุฃูุช ูุณุงุนุฏ ุฐูู ูุชุฎุตุต ูู ุงูุฅุฌุงุจุฉ ุนู ุฃุณุฆูุฉ ุญูู ุฎุฏูุงุช ุงูููุชุจ ุงูุจุฑูุฌู. ุฃูุช ุฏูููุ ููุทููุ ููุงุฏุฑ ุนูู ุงูููู ูุงูุชูุณูุฑ. ูููุชู:

1. ุฅุฐุง ูุงู ุงูุณุคุงู ููุฌูุฏูุง ูู ุงูุณูุงู (context)ุ ุฃุฌุจ ุจุทุฑููุฉ ูุงุถุญุฉ ููููุนุฉุ ุญุชู ูู ูุงู ุงูุณุคุงู ูุฎุชุตุฑูุง ุฃู ุงุณุชุฎุฏู ูููุงุช ูุดุงุจูุฉ.
2. ุฅุฐุง ูู ููู ุงูุณุคุงู ููุฌูุฏูุง ูุตููุง ูู ุงููุณุชูุฏุงุชุ ุญุงูู ุดุฑุญ ุงูุฅุฌุงุจุฉ ุจูุงุกู ุนูู ุงููุนุฑูุฉ ุงูุนุงูุฉ ุนู ุตูุงุนุฉ ุงูุจุฑูุฌูุงุชุ ูุน ุงูุฅุดุงุฑุฉ ุฅูู ุฃููุง ูู ุงููุนุฑูุฉ ุงูุนุงูุฉ ูููุณุช ูู ุงููุณุชูุฏุงุช.
3. ุงุฌุนู ุฅุฌุงุจุชู ูููููุฉ ูุณููุฉุ ูุตูุฑุฉ ููุงุถุญุฉ ููููุนุฉ.
4. ูุง ุชุฎุชูู ูุนูููุงุช ุบูุฑ ุตุญูุญุฉ.
5. ุจุนุฏ ูู ุฅุฌุงุจุฉุ ุงูุชุฑุญ ุณุคุงู ูุชุงุจุนุฉ ูุตูุฑ ูููุงุฆู ูุชุนูู ุจุฎุฏูุงุช ุงูููุชุจ.
6. ุฅุฐุง ูู ุชุนุฑู ุงูุฅุฌุงุจุฉุ ุฃุฌุจ: "ูุง ุชูุฌุฏ ูุนูููุงุช ูุชุงุญุฉ ูู ุงููุณุชูุฏุงุช ุญูู ูุฐุง ุงูููุถูุน".

ุงูุณูุงู (ุงููุณุชูุฏุงุช):
{context}

ุณุคุงู ุงููุณุชุฎุฏู:
{input}

ุฅุฌุงุจุฉ (ูุน ุงูุชุฑุงุญ ุณุคุงู ูุชุงุจุนุฉ ูู ุงูููุงูุฉ):
"""
)

def main():
    st.title("๐ค ูุณุงุนุฏ ุงูููุชุจ ุงูุจุฑูุฌู ุงูุฐูู")
    
    # ุนุฑุถ ุงูุฃุณุฆูุฉ ุงูููุชุฑุญุฉ ูู ุงูุดุฑูุท ุงูุฌุงูุจู
    with st.sidebar:
        st.markdown("### ๐ก ุฃุณุฆูุฉ ููุชุฑุญุฉ")
        st.markdown("ุงุฎุชุฑ ุฃุญุฏ ุงูุฃุณุฆูุฉ ุฃุฏูุงู ุฃู ุงูุชุจ ุณุคุงูู ุงูุฎุงุต:")
        
        selected_question = st.selectbox(
            "ุงุฎุชุฑ ุณุคุงู:",
            options=[""] + SUGGESTED_QUESTIONS,
            label_visibility="collapsed"
        )

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ุนุฑุถ ุงููุญุงุฏุซุงุช ุงูุณุงุจูุฉ
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # ุฅุฐุง ุชู ุงุฎุชูุงุฑ ุณุคุงู ูู ุงููุงุฆูุฉุ ุงุณุชุฎุฏูู
    if selected_question:
        user_prompt = selected_question
    else:
        user_prompt = st.chat_input("ุงูุชุจ ุณุคุงูู ุฃู ุชุญูุชู ููุง...")
    if user_prompt:
        st.chat_message("user").markdown(user_prompt)
        st.session_state.messages.append({"role": "user", "content": user_prompt})
        
        if "ุตุจุงุญ ุงูุฎูุฑ" in user_prompt:
            greeting_reply = "ุตุจุงุญ ุงูููุฑ! ููู ูููููู ูุณุงุนุฏุชู ุงููููุ"
        elif "ูุณุงุก ุงูุฎูุฑ" in user_prompt:
            greeting_reply = "ูุณุงุก ุงูููุฑ! ููู ูููููู ูุณุงุนุฏุชู ุงููููุ"
        elif any(greet in user_prompt for greet in GREETINGS):
            greeting_reply = "ูุฑุญุจุง! ููู ูููููู ูุณุงุนุฏุชู ุงููููุ"
        else:
            greeting_reply = None

        if greeting_reply:
            st.chat_message("assistant").markdown(greeting_reply)
            st.session_state.messages.append({"role": "assistant", "content": greeting_reply})
        else:
            try:
                # ุงูุชุญูู ูู ูุชุบูุฑุงุช ุงูุจูุฆุฉ ุงูุฃุณุงุณูุฉ ูุจู ุงุณุชุฏุนุงุก ุงูููุงุฐุฌ
                if not HF_TOKEN:
                    st.error("ูุชุบูุฑ ุงูุจูุฆุฉ HF_TOKEN ุบูุฑ ูุถุจูุท. ูุฑุฌู ุถุจุทู ูุจู ุชุดุบูู ุงูุชุทุจูู.")
                    return
                if not GROQ_API_KEY:
                    st.error("ูุชุบูุฑ ุงูุจูุฆุฉ GROQ_API_KEY ุบูุฑ ูุถุจูุท. ูุฑุฌู ุถุจุทู ูุจู ุชุดุบูู ุงูุชุทุจูู.")
                    return

                # ูุญุงููุฉ ุชุญููู ูุงุนุฏุฉ FAISS ูุน ุฑุณุงูุฉ ูุงุถุญุฉ ุนูุฏ ุนุฏู ุชููุฑูุง
                try:
                    db = get_vectorstore()
                except FileNotFoundError as e:
                    st.error(str(e))
                    st.info(
                        "ูุชูููุฏ ูุงุนุฏุฉ FAISS ูุญูููุง ูููุฐ: python faiss_build.py "
                        "ุซู ุงุฑูุน ูุฌูุฏ vectorstore/db_faiss ุฅูู GitHub/Railway."
                    )
                    return

                llm = ChatGroq(
                    model="llama-3.1-8b-instant",
                    temperature=0.0,
                    max_tokens=512,
                    api_key=GROQ_API_KEY
                )

                # ุฅูุดุงุก ุณูุณูุฉ RAG ูุน ุงูุจุญุซ ุงูููุณุน
                combine_chain = create_stuff_documents_chain(llm, retrieval_prompt)
                rag_chain = create_retrieval_chain(db.as_retriever(search_kwargs={"k": 10}), combine_chain)

                # ุงุณุชุฏุนุงุก RAG
                result = rag_chain.invoke({"input": user_prompt})

                # ุงุณุชุฎุฑุงุฌ ุงููุต ุงููุนูู ููุท
                if isinstance(result, dict):
                    answer = result.get("output_text") or result.get("text")
                elif hasattr(result, "content"):
                    answer = result.content
                else:
                    answer = str(result)

                # ุฅุฐุง ูู ุชูุฌุฏ ูุชูุฌุฉ ูู ุงููุณุชูุฏุงุช โ ูุณุชุฎุฏู LLM ูููุนุฑูุฉ ุงูุนุงูุฉ
                if not answer or "ูุง ุชูุฌุฏ ูุนูููุงุช ูุชุงุญุฉ" in answer:
                    llm_response = llm.invoke(
                        f"ุฃุฌุจ ุนูู ุงูุณุคุงู ุงูุชุงูู ุญูู ุงูุฎุฏูุงุช ุงูุจุฑูุฌูุฉ ุจุทุฑููุฉ ูุงุถุญุฉ ููุจุณุทุฉ ูููุณุชุฎุฏู ุงูุนุงุฏู. "
                        f"ุฅุฐุง ูู ุชุนุฑู ุงูุฅุฌุงุจุฉ ุจุฏูุฉุ ุฃุฌุจ 'ูุง ุชูุฌุฏ ูุนูููุงุช ูุชุงุญุฉ': {user_prompt}"
                    )
                    if hasattr(llm_response, "content"):
                        answer = llm_response.content
                    else:
                        answer = str(llm_response)
                    answer = f"ููุงุญุธุฉ: ูุฐู ุงููุนูููุงุช ูู ุงููุนุฑูุฉ ุงูุนุงูุฉ. {answer}"

                st.chat_message("assistant").markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"ุญุฏุซ ุฎุทุฃ ุฏุงุฎูู: {repr(e)}")
                st.code(traceback.format_exc())

if __name__ == "__main__":
    main()

