import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEndpointEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain import hub
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain


load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_FAISS_PATH = os.path.join(BASE_DIR, "vectorstore", "db_faiss")


def build_rag_chain():
    """ØªÙ‡ÙŠØ¦Ø© Ø³Ù„Ø³Ù„Ø© RAG Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ø§Ø¹Ø¯Ø© FAISS Ù…Ù† Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±."""

    if not HF_TOKEN:
        raise RuntimeError("HF_TOKEN ØºÙŠØ± Ù…Ø¶Ø¨ÙˆØ· ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©.")
    if not GROQ_API_KEY:
        raise RuntimeError("GROQ_API_KEY ØºÙŠØ± Ù…Ø¶Ø¨ÙˆØ· ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©.")

    if not os.path.isdir(DB_FAISS_PATH):
        raise FileNotFoundError(
            f"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© FAISS ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {DB_FAISS_PATH}. "
            "Ø´ØºÙ‘Ù„ faiss_build.py Ø£ÙˆÙ„Ø§Ù‹ Ù„Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§."
        )

    embeddings = HuggingFaceEndpointEmbeddings(
        model="sentence-transformers/all-MiniLM-L6-v2",
        task="feature-extraction",
        huggingfacehub_api_token=HF_TOKEN,
    )

    db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)

    llm = ChatGroq(
        model="llama-3.1-70b-versatile",
        temperature=0.1,
        max_tokens=512,
        api_key=GROQ_API_KEY,
    )

    prompt = hub.pull("langchain-ai/retrieval-qa-chat")
    combine_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(db.as_retriever(search_kwargs={"k": 5}), combine_chain)
    return rag_chain


def main():
    try:
        rag_chain = build_rag_chain()
    except Exception as e:
        print(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø³Ù„Ø³Ù„Ø© RAG: {e}")
        return

    user_query = input("ðŸ’¬ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø­ÙˆÙ„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù‡Ù†Ø§: ")
    if not user_query:
        print("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ø£ÙŠ Ø³Ø¤Ø§Ù„.")
        return

    response = rag_chain.invoke({"input": user_query})

    answer = None
    if isinstance(response, dict):
        answer = (
            response.get("answer")
            or response.get("output_text")
            or response.get("text")
        )
    else:
        answer = str(response)

    print("\nðŸ¤– Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
    if answer is not None:
        print(answer)
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ø³Ù„Ø³Ù„Ø©.")

    if isinstance(response, dict) and response.get("context"):
        print("\nðŸ“š Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØµØ¯Ø±ÙŠØ©:")
        for doc in response["context"]:
            print(f"- {doc.metadata} -> {doc.page_content[:200]}...")


if __name__ == "__main__":
    main()
